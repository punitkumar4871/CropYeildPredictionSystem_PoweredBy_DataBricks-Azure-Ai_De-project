{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8df68b1-3fa6-4f58-bcb6-26d926333d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1=pd.read_csv(\"yeild_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995ae432-190d-4823-bdf0-99a534bbdb25",
   "metadata": {},
   "source": [
    "# `Fine-Tuned XGBoost Regression for Crop Yield Prediction`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4461f5f-ce28-4b7b-970f-a5c3ac60f482",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [06:42:30] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [06:42:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\core.py:729: UserWarning: [06:42:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model performance on training Data\n",
      "Train R² Score: 0.9897809535924578\n",
      "Train MAE: 6.276916410238808\n",
      "Train MSE: 6083.879010159382\n",
      "Train RMSE: 77.99922442024268\n",
      "\n",
      "model performance on test Data\n",
      "Test R² Score: 0.9591218610121297\n",
      "Test MAE: 9.674200238811087\n",
      "Test MSE: 32910.83043797658\n",
      "Test RMSE: 181.4134240842628\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "a = df1.drop(columns=['yeild', 'Production','Soil pH','Soil Type'], axis=1) \n",
    "b = df1['yeild']  \n",
    "\n",
    "\n",
    "# step1Identify Numerical & Categorical Columns\n",
    "num_features = a.select_dtypes(exclude=\"object\").columns\n",
    "cat_features = a.select_dtypes(include=\"object\").columns\n",
    "\n",
    "# Step2 Create Column Transformer for Preprocessing\n",
    "numeric_transformer = StandardScaler()\n",
    "oh_transformer = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"OneHotEncoder\", oh_transformer, cat_features),\n",
    "        (\"StandardScaler\", numeric_transformer, num_features),        \n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "#Step3 Split the Data (Avoid Data Leakage)\n",
    "a_train, a_test, b_train, b_test = train_test_split(a, b, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "#Step4 Apply Preprocessing (Fit Only on Training Data)\n",
    "a_train = preprocessor.fit_transform(a_train)\n",
    "a_test = preprocessor.transform(a_test)\n",
    "\n",
    "\n",
    "# Step 5 train xgBoost with hyperparamters\n",
    "xgb = XGBRegressor(\n",
    "    random_state=42, \n",
    "    tree_method='gpu_hist', \n",
    "    n_estimators=300,           \n",
    "    learning_rate=0.05,         \n",
    "    max_depth=8,                \n",
    "    subsample=0.8,              \n",
    "    colsample_bytree=0.8,       \n",
    "    reg_alpha=0.3,              \n",
    "    reg_lambda=0.8              \n",
    ")\n",
    "\n",
    "# Train the Model\n",
    "xgb.fit(a_train, b_train)\n",
    "\n",
    "# predictions & evaluation\n",
    "y_train_pred = xgb.predict(a_train)\n",
    "y_test_pred = xgb.predict(a_test)\n",
    "\n",
    "# training metrics\n",
    "train_mae = mean_absolute_error(b_train, y_train_pred)\n",
    "train_mse = mean_squared_error(b_train, y_train_pred)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_r2 = r2_score(b_train, y_train_pred)\n",
    "# test metrics\n",
    "test_mae = mean_absolute_error(b_test, y_test_pred)\n",
    "test_mse = mean_squared_error(b_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_r2 = r2_score(b_test, y_test_pred)\n",
    "\n",
    "# final output\n",
    "print(\"model performance on training Data\")\n",
    "print(f\"Train R² Score: {train_r2}\")\n",
    "print(f\"Train MAE: {train_mae}\")\n",
    "print(f\"Train MSE: {train_mse}\")\n",
    "print(f\"Train RMSE: {train_rmse}\\n\")\n",
    "print(\"model performance on test Data\")\n",
    "print(f\"Test R² Score: {test_r2}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b12e1b-1f92-4624-a43f-7d2381c42918",
   "metadata": {},
   "source": [
    "# `Hyperparameter-Tuned XGBoost Regression for Crop Yield Prediction With RandomizedSearch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "171a2524-86dd-455c-a5a0-f6141775659e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [07:52:51] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [07:53:09] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [07:53:26] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " best hyperparameters found: {'subsample': 0.8, 'reg_lambda': 0.7, 'reg_alpha': 0.5, 'n_estimators': 300, 'min_child_weight': 5, 'max_depth': 10, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n",
      "\n",
      " Model Performance on Training Data:\n",
      "Train R² Score: 0.9780648383862353\n",
      "Train MAE: 8.710967162325291\n",
      "Train MSE: 13059.033495331125\n",
      "Train RMSE: 114.27612828290572\n",
      "\n",
      "model performance on test data:\n",
      "Test R² Score: 0.9612977378857608\n",
      "Test MAE: 11.448073280404685\n",
      "Test MSE: 31159.040444228645\n",
      "Test RMSE: 176.51923533776323\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "a = df1.drop(columns=['yeild', 'Production'], axis=1)\n",
    " # Target variable\n",
    "b = df1['yeild'] \n",
    "\n",
    "# Step 1 identify numerical & categorical columns\n",
    "num_features = a.select_dtypes(exclude=\"object\").columns\n",
    "cat_features = a.select_dtypes(include=\"object\").columns\n",
    "\n",
    "# Step 2: create column transformer for preprocessing\n",
    "numeric_transformer = StandardScaler()\n",
    "oh_transformer = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"OneHotEncoder\", oh_transformer, cat_features),\n",
    "        (\"StandardScaler\", numeric_transformer, num_features),        \n",
    "    ]\n",
    ")\n",
    "\n",
    "# Step3 split the data (avoid data leakage)\n",
    "a_train, a_test, b_train, b_test = train_test_split(a, b, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4 apply preprocessing (fit only on training data)\n",
    "a_train = preprocessor.fit_transform(a_train)\n",
    "a_test = preprocessor.transform(a_test)\n",
    "\n",
    "# step 5 define xgBoost regressor\n",
    "xgb = XGBRegressor(tree_method='gpu_hist', objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# Step 6 define hyperparameter grid for tuning**\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 300, 500, 800],\n",
    "    'learning_rate': [0.01, 0.03, 0.05, 0.1],\n",
    "    'max_depth': [4, 6, 7, 10],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.85, 1.0],\n",
    "    'reg_alpha': [0.1, 0.2, 0.3, 0.5],\n",
    "    'reg_lambda': [0.5, 0.7, 0.8, 1.0],\n",
    "    'min_child_weight': [1, 3, 5]\n",
    "}\n",
    "\n",
    "# step 7 hyperparameter tuning using randomizedsearchv\n",
    "xgb_grid = RandomizedSearchCV(\n",
    "    xgb, param_grid, cv=5, scoring='r2', n_iter=30, n_jobs=-1, verbose=1, random_state=42\n",
    ")\n",
    "xgb_grid.fit(a_train, b_train)\n",
    "\n",
    "# Step 8 Train the Best Model\n",
    "best_model = xgb_grid.best_estimator_\n",
    "best_model.fit(a_train, b_train)\n",
    "\n",
    "# Step 9 predictions & evaluation\n",
    "y_train_pred = best_model.predict(a_train)\n",
    "y_test_pred = best_model.predict(a_test)\n",
    "\n",
    "# training metrics\n",
    "train_mae = mean_absolute_error(b_train, y_train_pred)\n",
    "train_mse = mean_squared_error(b_train, y_train_pred)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_r2 = r2_score(b_train, y_train_pred)\n",
    "# test metrics\n",
    "test_mae = mean_absolute_error(b_test, y_test_pred)\n",
    "test_mse = mean_squared_error(b_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_r2 = r2_score(b_test, y_test_pred)\n",
    "# Step10 Final Output\n",
    "print(\" best hyperparameters found:\", xgb_grid.best_params_)\n",
    "print(\"\\n Model Performance on Training Data:\")\n",
    "print(f\"Train R² Score: {train_r2}\")\n",
    "print(f\"Train MAE: {train_mae}\")\n",
    "print(f\"Train MSE: {train_mse}\")\n",
    "print(f\"Train RMSE: {train_rmse}\\n\")\n",
    "print(\"model performance on test data:\")\n",
    "print(f\"Test R² Score: {test_r2}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7301db1-182f-4e46-a8fe-6a722fca52ea",
   "metadata": {},
   "source": [
    "# `Hyperparameter-Tuned XGBoost Regression for Crop Yield Prediction With RandomizedSearch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "942201d5-c852-4025-b3ea-a0204e6c6a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:59:13] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:59:35] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "D:\\anaconda\\Lib\\site-packages\\xgboost\\core.py:2676: UserWarning: [08:59:54] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " best hyperparameters found: {'subsample': 0.8, 'reg_lambda': 0.5, 'reg_alpha': 0.1, 'n_estimators': 500, 'min_child_weight': 3, 'max_depth': 7, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n",
      "\n",
      " Model Performance on Training Data:\n",
      "Train R² Score: 0.9849841862987546\n",
      "Train MAE: 8.287516381375122\n",
      "Train MSE: 8939.620210555639\n",
      "Train RMSE: 94.54956483535838\n",
      "\n",
      "model performance on test data:\n",
      "Test R² Score: 0.9599103353794801\n",
      "Test MAE: 11.721002978185059\n",
      "Test MSE: 32276.032796717507\n",
      "Test RMSE: 179.65531663916187\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "a = df1.drop(columns=['yeild', 'Production','Soil pH','Soil Type'], axis=1)\n",
    " # Target variable\n",
    "b = df1['yeild'] \n",
    "\n",
    "# Step 1 identify numerical & categorical columns\n",
    "num_features = a.select_dtypes(exclude=\"object\").columns\n",
    "cat_features = a.select_dtypes(include=\"object\").columns\n",
    "\n",
    "# Step 2: create column transformer for preprocessing\n",
    "numeric_transformer = StandardScaler()\n",
    "oh_transformer = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"OneHotEncoder\", oh_transformer, cat_features),\n",
    "        (\"StandardScaler\", numeric_transformer, num_features),        \n",
    "    ]\n",
    ")\n",
    "\n",
    "# Step3 split the data (avoid data leakage)\n",
    "a_train, a_test, b_train, b_test = train_test_split(a, b, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4 apply preprocessing (fit only on training data)\n",
    "a_train = preprocessor.fit_transform(a_train)\n",
    "a_test = preprocessor.transform(a_test)\n",
    "\n",
    "# step 5 define xgBoost regressor\n",
    "xgb = XGBRegressor(tree_method='gpu_hist', objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# Step 6 define hyperparameter grid for tuning**\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 300, 500, 800],\n",
    "    'learning_rate': [0.01, 0.03, 0.05, 0.1],\n",
    "    'max_depth': [4, 6, 7, 10],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.85, 1.0],\n",
    "    'reg_alpha': [0.1, 0.2, 0.3, 0.5],\n",
    "    'reg_lambda': [0.5, 0.7, 0.8, 1.0],\n",
    "    'min_child_weight': [1, 3, 5]\n",
    "}\n",
    "\n",
    "# step 7 hyperparameter tuning using randomizedsearchv\n",
    "xgb_grid = RandomizedSearchCV(\n",
    "    xgb, param_grid, cv=5, scoring='r2', n_iter=30, n_jobs=-1, verbose=1, random_state=42\n",
    ")\n",
    "xgb_grid.fit(a_train, b_train)\n",
    "\n",
    "# Step 8 Train the Best Model\n",
    "best_model = xgb_grid.best_estimator_\n",
    "best_model.fit(a_train, b_train)\n",
    "\n",
    "# Step 9 predictions & evaluation\n",
    "y_train_pred = best_model.predict(a_train)\n",
    "y_test_pred = best_model.predict(a_test)\n",
    "\n",
    "# training metrics\n",
    "train_mae = mean_absolute_error(b_train, y_train_pred)\n",
    "train_mse = mean_squared_error(b_train, y_train_pred)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_r2 = r2_score(b_train, y_train_pred)\n",
    "# test metrics\n",
    "test_mae = mean_absolute_error(b_test, y_test_pred)\n",
    "test_mse = mean_squared_error(b_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_r2 = r2_score(b_test, y_test_pred)\n",
    "# Step10 Final Output\n",
    "print(\" best hyperparameters found:\", xgb_grid.best_params_)\n",
    "print(\"\\n Model Performance on Training Data:\")\n",
    "print(f\"Train R² Score: {train_r2}\")\n",
    "print(f\"Train MAE: {train_mae}\")\n",
    "print(f\"Train MSE: {train_mse}\")\n",
    "print(f\"Train RMSE: {train_rmse}\\n\")\n",
    "print(\"model performance on test data:\")\n",
    "print(f\"Test R² Score: {test_r2}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
